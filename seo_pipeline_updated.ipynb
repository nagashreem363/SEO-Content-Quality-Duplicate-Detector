{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93f85c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3fed174",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 0: INSTALL DEPENDENCIES\n",
    "# =========================\n",
    "try:\n",
    "    get_ipython  \n",
    "    IN_NOTEBOOK = True\n",
    "except Exception:\n",
    "    IN_NOTEBOOK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1527db45",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def pip_install(pkg_line):\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkg_line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca1de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pkgs in [\n",
    "    \"beautifulsoup4==4.12.3\",\n",
    "    \"requests==2.32.3\",\n",
    "    \"pandas==2.2.2\",\n",
    "    \"numpy==1.26.4\",\n",
    "    \"scikit-learn==1.4.2\",\n",
    "    \"textstat==0.7.4\",\n",
    "    \"tqdm==4.66.4\",\n",
    "    \"lxml==5.2.2\",\n",
    "    \"gdown==5.1.0\",\n",
    "]:\n",
    "    pip_install(pkgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc04d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a277fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5add2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2511434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19744677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd7a2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52691b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff32a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"seo-content-detector\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "NOTEBOOKS_DIR = os.path.join(BASE_DIR, \"notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f628fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(NOTEBOOKS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24af5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"duplicate_threshold\": 0.80,\n",
    "    \"similarity_threshold_realtime\": 0.75,\n",
    "    \"thin_content_threshold\": 500,\n",
    "    \"keywords_top_n\": 5,\n",
    "    \"tfidf_max_features_keywords\": 5000,\n",
    "    \"tfidf_max_features_embeddings\": 2000,\n",
    "    \"scrape_user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "    \"scrape_timeout\": 20,\n",
    "    \"scrape_delay_min_sec\": 1.0,\n",
    "    \"scrape_delay_max_sec\": 2.0,\n",
    "    \"train_test_split_ratio\": 0.30,\n",
    "    \"random_state\": 42,\n",
    "    \"alternative_dataset_gdrive_id\": \"1q-49ey_ydbB1TnN5x60K9VhyoM0vDI1K\",\n",
    "}\n",
    "random.seed(CONFIG[\"random_state\"])\n",
    "np.random.seed(CONFIG[\"random_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce8b793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, \"config.json\"), \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9584763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config saved to: seo-content-detector\\config.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Config saved to:\", os.path.join(BASE_DIR, \"config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27590719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 2: DATA LOADING / DOWNLOADING\n",
    "# =========================\n",
    "def ensure_dataset():\n",
    "    \"\"\"\n",
    "    Use local seo-content-detector/data/data.csv if present.\n",
    "    Otherwise auto-download the alternative URLs-only dataset via Google Drive (gdown).\n",
    "    Expected columns:\n",
    "      - Primary: url, html_content\n",
    "      - Alternative: url\n",
    "    \"\"\"\n",
    "    target_csv = os.path.join(DATA_DIR, \"data.csv\")\n",
    "    if os.path.exists(target_csv):\n",
    "        df = pd.read_csv(target_csv)\n",
    "        print(f\"Found dataset at {target_csv} (rows={len(df)})\")\n",
    "        return df\n",
    "\n",
    "    print(\"data.csv not found. Attempting to download alternative URLs-only dataset via Google Drive...\")\n",
    "    import gdown\n",
    "    g_url = f\"https://drive.google.com/uc?id={CONFIG['alternative_dataset_gdrive_id']}\"\n",
    "    alt_path = os.path.join(DATA_DIR, \"urls_only.csv\")\n",
    "    gdown.download(g_url, alt_path, quiet=False)\n",
    "    dfa = pd.read_csv(alt_path)\n",
    "\n",
    "    # Normalize column to 'url'\n",
    "    if \"url\" not in dfa.columns:\n",
    "        for cand in [\"URL\", \"Url\", \"link\", \"Link\", \"urls\", \"Urls\", \"Links\"]:\n",
    "            if cand in dfa.columns:\n",
    "                dfa = dfa.rename(columns={cand: \"url\"})\n",
    "                break\n",
    "    if \"url\" not in dfa.columns:\n",
    "        raise RuntimeError(\"Downloaded alternative dataset doesn't contain a 'url' column.\")\n",
    "\n",
    "    dfa.to_csv(target_csv, index=False)\n",
    "    print(f\"Alternative dataset normalized and saved to {target_csv} (rows={len(dfa)})\")\n",
    "    return dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e962942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_html_column(df):\n",
    "    cols = list(df.columns)\n",
    "    lc = [c.lower() for c in cols]\n",
    "    for needle in [\"html_content\", \"html\", \"raw_html\", \"content_html\"]:\n",
    "        if needle in lc:\n",
    "            return cols[lc.index(needle)]\n",
    "    for i, name in enumerate(lc):\n",
    "        if \"html\" in name:\n",
    "            return cols[i]\n",
    "    for i, name in enumerate(lc):\n",
    "        if \"content\" in name:\n",
    "            return cols[i]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed1b2bf6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 3: SESSION, SCRAPING & HTML PARSING\n",
    "# =========================\n",
    "def make_session():\n",
    "    sess = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.6,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    sess.mount(\"http://\", adapter)\n",
    "    sess.mount(\"https://\", adapter)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db654ed8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "SESSION = make_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0599c99a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scrape_url(url, headers=None, timeout=15):\n",
    "    headers = headers or {\"User-Agent\": CONFIG[\"scrape_user_agent\"], \"Accept-Language\": \"en-US,en;q=0.9\"}\n",
    "    try:\n",
    "        resp = SESSION.get(url, headers=headers, timeout=timeout)\n",
    "        if resp.status_code == 200 and resp.text:\n",
    "            return resp.text\n",
    "        return None\n",
    "    except requests.RequestException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4dadf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_text(html):\n",
    "    \"\"\"\n",
    "    Extract title and main content with heuristics.\n",
    "    \"\"\"\n",
    "    if not html or not isinstance(html, str):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        # Title\n",
    "        title_tag = soup.find(\"title\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "        # Remove boilerplate\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\", \"aside\", \"form\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        # Preferred containers\n",
    "        candidates = []\n",
    "        selectors = [\"article\", \"main\", '[role=\"main\"]', \"div#content\", \"div.content\", \"div.post\", \"section\"]\n",
    "        for sel in selectors:\n",
    "            for node in soup.select(sel):\n",
    "                txt = node.get_text(separator=\" \", strip=True)\n",
    "                wc = len(txt.split())\n",
    "                candidates.append((txt, wc))\n",
    "\n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            body_text = candidates[0][0]\n",
    "        else:\n",
    "            # Fallback: p/h*/li\n",
    "            texts = []\n",
    "            for tag in soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"li\"]):\n",
    "                t = tag.get_text(\" \", strip=True)\n",
    "                if t:\n",
    "                    texts.append(t)\n",
    "            body_text = \" \".join(texts)\n",
    "\n",
    "        # Final fallback if still short\n",
    "        if len(body_text.split()) < 30:\n",
    "            full_text = soup.get_text(separator=\" \", strip=True)\n",
    "            if len(full_text.split()) > len(body_text.split()):\n",
    "                body_text = full_text\n",
    "\n",
    "        body_text = re.sub(r\"\\s+\", \" \", body_text).strip()\n",
    "        return title, body_text\n",
    "    except Exception:\n",
    "        return \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6351ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_primary_dataset(df, html_col):\n",
    "    rows = []\n",
    "    empty_count = 0\n",
    "    for _, r in tqdm(df.iterrows(), total=len(df), desc=\"Parsing HTML (primary)\"):\n",
    "        url = r.get(\"url\", \"\")\n",
    "        html = r.get(html_col, None)\n",
    "        title, body_text = extract_main_text(html)\n",
    "        wc = len(body_text.split()) if body_text else 0\n",
    "        if wc == 0:\n",
    "            empty_count += 1\n",
    "        rows.append({\"url\": url, \"title\": title, \"body_text\": body_text, \"word_count\": wc})\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out_path = os.path.join(DATA_DIR, \"extracted_content.csv\")\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(f\"Saved extracted content to {out_path} (rows={len(out)})\")\n",
    "    print(f\"Empty/zero-word pages (primary): {empty_count}/{len(out)}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d897ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_alternative_dataset_and_scrape(df_urls):\n",
    "    assert \"url\" in df_urls.columns, \"Alternative dataset must contain 'url' column.\"\n",
    "    rows = []\n",
    "    ok = 0\n",
    "    for _, r in tqdm(df_urls.iterrows(), total=len(df_urls), desc=\"Scraping & parsing (alternative)\"):\n",
    "        url = str(r[\"url\"])\n",
    "        html = scrape_url(url, headers={\"User-Agent\": CONFIG[\"scrape_user_agent\"], \"Accept-Language\": \"en-US,en;q=0.9\"}, timeout=CONFIG[\"scrape_timeout\"])\n",
    "        if html:\n",
    "            title, body_text = extract_main_text(html)\n",
    "        else:\n",
    "            title, body_text = \"\", \"\"\n",
    "        wc = len(body_text.split()) if body_text else 0\n",
    "        ok += (wc > 0)\n",
    "        rows.append({\"url\": url, \"title\": title, \"body_text\": body_text, \"word_count\": wc})\n",
    "        time.sleep(random.uniform(CONFIG[\"scrape_delay_min_sec\"], CONFIG[\"scrape_delay_max_sec\"]))\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out_path = os.path.join(DATA_DIR, \"extracted_content.csv\")\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(f\"Saved extracted content to {out_path} (rows={len(out)}, non-empty={ok})\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faeada89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 4: TEXT CLEANING & FEATURES\n",
    "# =========================\n",
    "def clean_text_simple(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32d2d168",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def count_sentences(text: str) -> int:\n",
    "    if not text or not isinstance(text, str):\n",
    "        return 0\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    parts = [p for p in parts if p.strip()]\n",
    "    return len(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfe391d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_readability(s: str) -> float:\n",
    "    try:\n",
    "        return float(textstat.flesch_reading_ease(s or \"\"))\n",
    "    except Exception:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "876df543",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_keywords_tfidf(texts, top_n=5, max_features=5000):\n",
    "    if all((not t) or t.strip() == \"\" for t in texts):\n",
    "        return [\"\"] * len(texts), None\n",
    "    vec = TfidfVectorizer(max_features=max_features, stop_words=\"english\")\n",
    "    tfidf = vec.fit_transform(texts)\n",
    "    feature_names = np.array(vec.get_feature_names_out())\n",
    "    result = []\n",
    "    for i in range(tfidf.shape[0]):\n",
    "        row = tfidf.getrow(i).toarray().ravel()\n",
    "        if row.sum() == 0:\n",
    "            result.append(\"\")\n",
    "            continue\n",
    "        top_idx = row.argsort()[-top_n:][::-1]\n",
    "        kws = feature_names[top_idx]\n",
    "        result.append(\"|\".join(kws))\n",
    "    with open(os.path.join(MODELS_DIR, \"keywords_tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(vec, f)\n",
    "    return result, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "303fb810",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_embeddings_tfidf(texts, max_features=2000):\n",
    "    if all((not t) or t.strip() == \"\" for t in texts):\n",
    "        return None, None\n",
    "    vec = TfidfVectorizer(max_features=max_features, stop_words=\"english\")\n",
    "    X = vec.fit_transform(texts)\n",
    "    emb = X.toarray().astype(np.float32)\n",
    "    norms = np.linalg.norm(emb, axis=1, keepdims=True) + 1e-8\n",
    "    emb = emb / norms\n",
    "    with open(os.path.join(MODELS_DIR, \"tfidf_embed_vectorizer.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(vec, f)\n",
    "    return emb, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aa9648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(extracted_df):\n",
    "    df = extracted_df.copy()\n",
    "    df[\"clean_text\"] = df[\"body_text\"].fillna(\"\").astype(str).apply(clean_text_simple)\n",
    "    df[\"sentence_count\"] = df[\"body_text\"].fillna(\"\").astype(str).apply(count_sentences)\n",
    "    df[\"flesch_reading_ease\"] = df[\"body_text\"].fillna(\"\").astype(str).apply(compute_readability)\n",
    "\n",
    "    # Keywords\n",
    "    df[\"top_keywords\"], _ = extract_keywords_tfidf(\n",
    "        df[\"clean_text\"].tolist(),\n",
    "        top_n=CONFIG[\"keywords_top_n\"],\n",
    "        max_features=CONFIG[\"tfidf_max_features_keywords\"]\n",
    "    )\n",
    "\n",
    "    # Embeddings\n",
    "    embeddings, embed_vec = compute_embeddings_tfidf(\n",
    "        df[\"clean_text\"].tolist(),\n",
    "        max_features=CONFIG[\"tfidf_max_features_embeddings\"]\n",
    "    )\n",
    "    if embeddings is None:\n",
    "        print(\"All texts are empty. Cannot compute embeddings. Aborting further steps.\")\n",
    "        df[\"embedding\"] = \"[]\"\n",
    "        out_path = os.path.join(DATA_DIR, \"features.csv\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f\"Saved features to {out_path} (rows={len(df)})\")\n",
    "        return df, None\n",
    "\n",
    "    df[\"embedding\"] = [json.dumps(v.tolist()) for v in embeddings]\n",
    "    out_path = os.path.join(DATA_DIR, \"features.csv\")\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved features to {out_path} (rows={len(df)})\")\n",
    "    return df, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93738b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 5: DUPLICATE DETECTION\n",
    "# =========================\n",
    "def detect_duplicates(features_df, embeddings, threshold=0.80):\n",
    "    if embeddings is None or len(embeddings) == 0:\n",
    "        print(\"No embeddings available. Skipping duplicate detection.\")\n",
    "        dup_df = pd.DataFrame(columns=[\"url1\",\"url2\",\"similarity\"])\n",
    "        dup_df.to_csv(os.path.join(DATA_DIR, \"duplicates.csv\"), index=False)\n",
    "        return dup_df\n",
    "\n",
    "    sim = cosine_similarity(embeddings)\n",
    "    n = sim.shape[0]\n",
    "\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            s = float(sim[i, j])\n",
    "            if s > threshold:\n",
    "                pairs.append({\n",
    "                    \"url1\": features_df.iloc[i][\"url\"],\n",
    "                    \"url2\": features_df.iloc[j][\"url\"],\n",
    "                    \"similarity\": round(s, 4)\n",
    "                })\n",
    "\n",
    "    dup_df = pd.DataFrame(pairs)\n",
    "    out_path = os.path.join(DATA_DIR, \"duplicates.csv\")\n",
    "    dup_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved duplicate pairs to {out_path} (pairs={len(dup_df)})\")\n",
    "\n",
    "    features_df[\"is_thin\"] = features_df[\"word_count\"] < CONFIG[\"thin_content_threshold\"]\n",
    "    features_df.to_csv(os.path.join(DATA_DIR, \"features.csv\"), index=False)\n",
    "\n",
    "    total = len(features_df)\n",
    "    thin_count = int(features_df[\"is_thin\"].sum())\n",
    "    print(\"Summary:\")\n",
    "    print(f\"- Total pages analyzed: {total}\")\n",
    "    print(f\"- Duplicate pairs: {len(dup_df)}\")\n",
    "    print(f\"- Thin content pages: {thin_count} ({100.0 * thin_count / max(1,total):.1f}%)\")\n",
    "\n",
    "    if len(dup_df) > 0:\n",
    "        print(\"Sample duplicates:\")\n",
    "        print(dup_df.head(5).to_string(index=False))\n",
    "\n",
    "    return dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f82ead0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 6: QUALITY MODEL\n",
    "# =========================\n",
    "def label_quality_rules(row):\n",
    "    wc = row[\"word_count\"]\n",
    "    rd = row[\"flesch_reading_ease\"]\n",
    "    if (wc > 1500) and (50 <= rd <= 70):\n",
    "        return \"High\"\n",
    "    if (wc < 500) or (rd < 30):\n",
    "        return \"Low\"\n",
    "    return \"Medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6708ddb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_baseline_prediction(word_count):\n",
    "    if word_count > 1500:\n",
    "        return \"High\"\n",
    "    if word_count < 500:\n",
    "        return \"Low\"\n",
    "    return \"Medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7218f513",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ensure_label_variety(df):\n",
    "    labels, counts = np.unique(df[\"quality_label\"].values, return_counts=True)\n",
    "    if len(labels) >= 2:\n",
    "        return df, \"rules\"\n",
    "    wc = df[\"word_count\"].values\n",
    "    rd = df[\"flesch_reading_ease\"].values\n",
    "    q80_wc = np.percentile(wc, 80)\n",
    "    q20_wc = np.percentile(wc, 20)\n",
    "    q60_rd = np.percentile(rd, 60)\n",
    "    new_labels = []\n",
    "    for i in range(len(df)):\n",
    "        if (wc[i] >= q80_wc) and (rd[i] >= q60_rd):\n",
    "            new_labels.append(\"High\")\n",
    "        elif (wc[i] <= q20_wc) or (rd[i] < 30):\n",
    "            new_labels.append(\"Low\")\n",
    "        else:\n",
    "            new_labels.append(\"Medium\")\n",
    "    df[\"quality_label\"] = new_labels\n",
    "    return df, \"quantile-fallback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a7d1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quality_model(features_df):\n",
    "    df = features_df.copy()\n",
    "    df[\"quality_label\"] = df.apply(label_quality_rules, axis=1)\n",
    "    df, label_strategy = ensure_label_variety(df)\n",
    "\n",
    "    feat_cols = [\"word_count\", \"sentence_count\", \"flesch_reading_ease\"]\n",
    "    X = df[feat_cols].values\n",
    "    y = df[\"quality_label\"].values\n",
    "\n",
    "    labels, counts = np.unique(y, return_counts=True)\n",
    "    can_stratify = all(c >= 2 for c in counts) and len(labels) >= 2\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=CONFIG[\"train_test_split_ratio\"],\n",
    "        random_state=CONFIG[\"random_state\"],\n",
    "        stratify=y if can_stratify else None\n",
    "    )\n",
    "\n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        X_train = np.vstack([X_train, X_test])\n",
    "        y_train = np.concatenate([y_train, y_test])\n",
    "\n",
    "    if len(np.unique(y_train)) < 2:\n",
    "        print(\"Not enough label variety to train classifier. Skipping ML model training.\")\n",
    "        return None, 0.0, 0.0, []\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=200, random_state=CONFIG[\"random_state\"])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    with open(os.path.join(MODELS_DIR, \"quality_model.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    labels_order = [\"Low\", \"Medium\", \"High\"]\n",
    "    rep = classification_report(y_test, y_pred, labels=labels_order, zero_division=0, digits=3)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels_order)\n",
    "\n",
    "    baseline_pred = [make_baseline_prediction(wc) for wc in X_test[:, 0]]\n",
    "    baseline_acc = accuracy_score(y_test, baseline_pred)\n",
    "\n",
    "    importances = list(zip(feat_cols, model.feature_importances_))\n",
    "    importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"Labeling strategy: {label_strategy}\")\n",
    "    print(\"Model Performance:\")\n",
    "    print(rep)\n",
    "    print(f\"Overall Accuracy: {acc:.3f}\")\n",
    "    print(f\"Baseline Accuracy (word_count only): {baseline_acc:.3f}\")\n",
    "    print(\"Confusion Matrix [Low, Medium, High]:\")\n",
    "    print(cm)\n",
    "    print(\"Top Features:\")\n",
    "    for name, imp in importances[:3]:\n",
    "        print(f\"- {name}: {imp:.3f}\")\n",
    "\n",
    "    return model, acc, baseline_acc, importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f965cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 7: REAL-TIME ANALYSIS\n",
    "# =========================\n",
    "def analyze_url(url):\n",
    "    \"\"\"\n",
    "    Scrapes URL, extracts features, predicts quality, and finds similar content.\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": CONFIG[\"scrape_user_agent\"], \"Accept-Language\": \"en-US,en;q=0.9\"}\n",
    "    try:\n",
    "        resp = SESSION.get(url, headers=headers, timeout=CONFIG[\"scrape_timeout\"])\n",
    "        resp.raise_for_status()\n",
    "        title, body_text = extract_main_text(resp.text)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to fetch URL: {e}\"}\n",
    "\n",
    "    if not body_text:\n",
    "        return {\"error\": \"No content extracted from the URL.\"}\n",
    "\n",
    "    clean = clean_text_simple(body_text)\n",
    "    word_count = len(body_text.split())\n",
    "    sentence_count = count_sentences(body_text)\n",
    "    readability = compute_readability(body_text)\n",
    "    is_thin = word_count < CONFIG[\"thin_content_threshold\"]\n",
    "\n",
    "    model_path = os.path.join(MODELS_DIR, \"quality_model.pkl\")\n",
    "    if not os.path.exists(model_path):\n",
    "        return {\"error\": \"quality_model.pkl not found. Run training first.\"}\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    features = np.array([[word_count, sentence_count, readability]], dtype=np.float32)\n",
    "    quality_label = model.predict(features)[0]\n",
    "\n",
    "    vec_path = os.path.join(MODELS_DIR, \"tfidf_embed_vectorizer.pkl\")\n",
    "    if not os.path.exists(vec_path):\n",
    "        return {\"error\": \"tfidf_embed_vectorizer.pkl not found. Run feature extraction first.\"}\n",
    "    with open(vec_path, \"rb\") as f:\n",
    "        embed_vec = pickle.load(f)\n",
    "\n",
    "    emb_query = embed_vec.transform([clean]).toarray().astype(np.float32)\n",
    "    emb_query = emb_query / (np.linalg.norm(emb_query, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    feat_path = os.path.join(DATA_DIR, \"features.csv\")\n",
    "    if not os.path.exists(feat_path):\n",
    "        return {\"error\": \"features.csv not found. Run the pipeline first.\"}\n",
    "    existing = pd.read_csv(feat_path)\n",
    "\n",
    "    try:\n",
    "        existing_embeddings = np.vstack([np.array(json.loads(x), dtype=np.float32) for x in existing[\"embedding\"].tolist()])\n",
    "    except Exception:\n",
    "        existing_embeddings = []\n",
    "        for e_str in existing[\"embedding\"].tolist():\n",
    "            try:\n",
    "                vec = json.loads(e_str)\n",
    "            except Exception:\n",
    "                vec = eval(e_str)\n",
    "            existing_embeddings.append(np.array(vec, dtype=np.float32))\n",
    "        existing_embeddings = np.vstack(existing_embeddings)\n",
    "\n",
    "    sims = cosine_similarity(emb_query, existing_embeddings)[0]\n",
    "    similar = []\n",
    "    for i, s in enumerate(sims):\n",
    "        if s > CONFIG[\"similarity_threshold_realtime\"]:\n",
    "            similar.append({\"url\": existing.iloc[i][\"url\"], \"similarity\": float(s)})\n",
    "    similar = sorted(similar, key=lambda d: d[\"similarity\"], reverse=True)[:5]\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"word_count\": int(word_count),\n",
    "        \"sentence_count\": int(sentence_count),\n",
    "        \"readability\": float(readability),\n",
    "        \"quality_label\": str(quality_label),\n",
    "        \"is_thin\": bool(is_thin),\n",
    "        \"similar_to\": similar,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90a5ffc8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STEP 8: RUN PIPELINE END-TO-END\n",
    "# =========================\n",
    "def run_pipeline():\n",
    "    print(\"=== SEO Content Quality & Duplicate Detector ===\")\n",
    "    # 1) Load dataset (local or auto-download alternative)\n",
    "    df_raw = ensure_dataset()\n",
    "    if len(df_raw) == 0:\n",
    "        raise RuntimeError(\"Dataset is empty.\")\n",
    "\n",
    "    df_raw.to_csv(os.path.join(DATA_DIR, \"data.csv\"), index=False)\n",
    "\n",
    "    # 2) Parse: primary or alternative\n",
    "    if \"url\" not in df_raw.columns:\n",
    "        raise RuntimeError(\"Dataset must contain 'url' column.\")\n",
    "    html_col = infer_html_column(df_raw)\n",
    "    if html_col and df_raw[html_col].notna().sum() > 0:\n",
    "        print(f\"Using primary dataset (HTML column='{html_col}')\")\n",
    "        df_content = parse_primary_dataset(df_raw, html_col)\n",
    "    else:\n",
    "        print(\"Using alternative dataset (URLs only). Scraping pages...\")\n",
    "        df_content = parse_alternative_dataset_and_scrape(df_raw[[\"url\"]])\n",
    "\n",
    "    if df_content[\"word_count\"].sum() == 0:\n",
    "        print(\"All extracted pages are empty. Check accessibility or provide primary dataset with html_content.\")\n",
    "        return\n",
    "\n",
    "    # 3) Features\n",
    "    features_df, embeddings = extract_features(df_content)\n",
    "    if embeddings is None:\n",
    "        print(\"No embeddings could be computed (empty text). Stopping after features.\")\n",
    "        return\n",
    "\n",
    "    # 4) Duplicates + thin content\n",
    "    dup_df = detect_duplicates(features_df, embeddings, threshold=CONFIG[\"duplicate_threshold\"])\n",
    "\n",
    "    # 5) Model training + evaluation\n",
    "    model, acc, baseline_acc, importances = train_quality_model(features_df)\n",
    "\n",
    "    # 6) Real-time demo (first URL)\n",
    "    test_url = features_df.iloc[0][\"url\"]\n",
    "    print(\"\\nReal-time analyze_url() demo on first dataset URL:\")\n",
    "    result = analyze_url(test_url)\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "    print(\"\\nPipeline complete. Outputs saved in seo-content-detector/data and models/\")\n",
    "    print(\"- data/extracted_content.csv\")\n",
    "    print(\"- data/features.csv\")\n",
    "    print(\"- data/duplicates.csv\")\n",
    "    print(\"- models/quality_model.pkl\")\n",
    "    print(\"- models/tfidf_embed_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8003e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SEO Content Quality & Duplicate Detector ===\n",
      "Found dataset at seo-content-detector\\data\\data.csv (rows=81)\n",
      "Using alternative dataset (URLs only). Scraping pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping & parsing (alternative): 100%|██████████| 81/81 [05:18<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted content to seo-content-detector\\data\\extracted_content.csv (rows=81, non-empty=72)\n",
      "Saved features to seo-content-detector\\data\\features.csv (rows=81)\n",
      "Saved duplicate pairs to seo-content-detector\\data\\duplicates.csv (pairs=8)\n",
      "Summary:\n",
      "- Total pages analyzed: 81\n",
      "- Duplicate pairs: 8\n",
      "- Thin content pages: 28 (34.6%)\n",
      "Sample duplicates:\n",
      "                                                                                                 url1                                                                                        url2  similarity\n",
      "                                         https://guardiandigital.com/resources/blog/guide-on-phishing                           https://inspiredelearning.com/blog/phishing-protection-checklist/      0.8019\n",
      "https://www.microsoft.com/en-us/security/business/security-101/what-is-zero-trust-network-access-ztna https://www.zscaler.com/resources/security-terms-glossary/what-is-zero-trust-network-access      0.8544\n",
      "                                                        https://sign.dropbox.com/products/dropbox-fax                                                                       https://www.fax.plus/      0.8186\n",
      "                        https://nytlicensing.com/latest/trends/content-marketing-best-practices-2022/                                                  https://copyblogger.com/content-marketing/      0.8279\n",
      "                        https://nytlicensing.com/latest/trends/content-marketing-best-practices-2022/                 https://www.twilio.com/en-us/blog/insights/content-marketing-best-practices      0.8227\n",
      "Labeling strategy: rules\n",
      "Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low      0.923     1.000     0.960        12\n",
      "      Medium      0.875     0.875     0.875         8\n",
      "        High      1.000     0.800     0.889         5\n",
      "\n",
      "    accuracy                          0.920        25\n",
      "   macro avg      0.933     0.892     0.908        25\n",
      "weighted avg      0.923     0.920     0.919        25\n",
      "\n",
      "Overall Accuracy: 0.920\n",
      "Baseline Accuracy (word_count only): 0.720\n",
      "Confusion Matrix [Low, Medium, High]:\n",
      "[[12  0  0]\n",
      " [ 1  7  0]\n",
      " [ 0  1  4]]\n",
      "Top Features:\n",
      "- flesch_reading_ease: 0.363\n",
      "- word_count: 0.329\n",
      "- sentence_count: 0.308\n",
      "\n",
      "Real-time analyze_url() demo on first dataset URL:\n",
      "{\n",
      "  \"url\": \"https://www.cm-alliance.com/cybersecurity-blog\",\n",
      "  \"title\": \"Cyber Security Blog\",\n",
      "  \"word_count\": 2011,\n",
      "  \"sentence_count\": 110,\n",
      "  \"readability\": 43.02,\n",
      "  \"quality_label\": \"Medium\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": [\n",
      "    {\n",
      "      \"url\": \"https://www.cm-alliance.com/cybersecurity-blog\",\n",
      "      \"similarity\": 0.9999998807907104\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Pipeline complete. Outputs saved in seo-content-detector/data and models/\n",
      "- data/extracted_content.csv\n",
      "- data/features.csv\n",
      "- data/duplicates.csv\n",
      "- models/quality_model.pkl\n",
      "- models/tfidf_embed_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Run everything\n",
    "run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
